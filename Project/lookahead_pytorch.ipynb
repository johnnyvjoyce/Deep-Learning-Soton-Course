{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "class Lookahead(Optimizer):\n",
    "    r\"\"\"PyTorch implementation of the lookahead wrapper.\n",
    "\n",
    "    Lookahead Optimizer: https://arxiv.org/abs/1907.08610\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, la_steps=5, la_alpha=0.8, pullback_momentum=\"none\"):\n",
    "        \"\"\"optimizer: inner optimizer\n",
    "        la_steps (int): number of lookahead steps\n",
    "        la_alpha (float): linear interpolation factor. 1.0 recovers the inner optimizer.\n",
    "        pullback_momentum (str): change to inner optimizer momentum on interpolation update\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self._la_step = 0  # counter for inner optimizer\n",
    "        self.la_alpha = la_alpha\n",
    "        self._total_la_steps = la_steps\n",
    "        pullback_momentum = pullback_momentum.lower()\n",
    "        assert pullback_momentum in [\"reset\", \"pullback\", \"none\"]\n",
    "        self.pullback_momentum = pullback_momentum\n",
    "\n",
    "        self.state = defaultdict(dict)\n",
    "\n",
    "        # Cache the current optimizer parameters\n",
    "        for group in optimizer.param_groups:\n",
    "            for p in group['params']:\n",
    "                param_state = self.state[p]\n",
    "                param_state['cached_params'] = torch.zeros_like(p.data)\n",
    "                param_state['cached_params'].copy_(p.data)\n",
    "                if self.pullback_momentum == \"pullback\":\n",
    "                    param_state['cached_mom'] = torch.zeros_like(p.data)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return {\n",
    "            'state': self.state,\n",
    "            'optimizer': self.optimizer,\n",
    "            'la_alpha': self.la_alpha,\n",
    "            '_la_step': self._la_step,\n",
    "            '_total_la_steps': self._total_la_steps,\n",
    "            'pullback_momentum': self.pullback_momentum\n",
    "        }\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def get_la_step(self):\n",
    "        return self._la_step\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.optimizer.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.optimizer.load_state_dict(state_dict)\n",
    "\n",
    "    def _backup_and_load_cache(self):\n",
    "        \"\"\"Useful for performing evaluation on the slow weights (which typically generalize better)\n",
    "        \"\"\"\n",
    "        for group in self.optimizer.param_groups:\n",
    "            for p in group['params']:\n",
    "                param_state = self.state[p]\n",
    "                param_state['backup_params'] = torch.zeros_like(p.data)\n",
    "                param_state['backup_params'].copy_(p.data)\n",
    "                p.data.copy_(param_state['cached_params'])\n",
    "\n",
    "    def _clear_and_load_backup(self):\n",
    "        for group in self.optimizer.param_groups:\n",
    "            for p in group['params']:\n",
    "                param_state = self.state[p]\n",
    "                p.data.copy_(param_state['backup_params'])\n",
    "                del param_state['backup_params']\n",
    "\n",
    "    @property\n",
    "    def param_groups(self):\n",
    "        return self.optimizer.param_groups\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single Lookahead optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = self.optimizer.step(closure)\n",
    "        self._la_step += 1\n",
    "\n",
    "        if self._la_step >= self._total_la_steps:\n",
    "            self._la_step = 0\n",
    "            # Lookahead and cache the current optimizer parameters\n",
    "            for group in self.optimizer.param_groups:\n",
    "                for p in group['params']:\n",
    "                    param_state = self.state[p]\n",
    "                    p.data.mul_(self.la_alpha).add_(1.0 - self.la_alpha, param_state['cached_params'])  # crucial line\n",
    "                    param_state['cached_params'].copy_(p.data)\n",
    "                    if self.pullback_momentum == \"pullback\":\n",
    "                        internal_momentum = self.optimizer.state[p][\"momentum_buffer\"]\n",
    "                        self.optimizer.state[p][\"momentum_buffer\"] = internal_momentum.mul_(self.la_alpha).add_(\n",
    "                            1.0 - self.la_alpha, param_state[\"cached_mom\"])\n",
    "                        param_state[\"cached_mom\"] = self.optimizer.state[p][\"momentum_buffer\"]\n",
    "                    elif self.pullback_momentum == \"reset\":\n",
    "                        self.optimizer.state[p][\"momentum_buffer\"] = torch.zeros_like(p.data)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#for alpha in np.arange(0.01,1.01,0.01): # 100 evenly spaced values of alpha in range (0,1]\n",
    "#    pass\n",
    "#    #Lookahead(la_steps=5, la_alpha=alpha)\n",
    "\n",
    "dim = 2\n",
    "A = np.array([[1,0],[0,1]])\n",
    "\n",
    "def noisyquadloss(x,A):\n",
    "    Sigma = np.linalg.inv(A)\n",
    "    c =np.dot(np.random.randn(2),Sigma)\n",
    "    return 1/2 * np.dot(np.dot(x-c,A), x-c)\n",
    "\n",
    "def best_var_sgd(lr,A):\n",
    "    I = np.identity.len(A)\n",
    "    Sigma = np.linalg.inv(A)\n",
    "    return lr**2 * A**2 * Sigma**2 / (I-(I-gamma*A)**2)\n",
    "\n",
    "def best_var_la(alpha,lr,A,k=5):\n",
    "    I = np.identity(len(A))\n",
    "    numerator = alpha**2 * (I - (I - lr*A)**(2*k))\n",
    "    denominator = numerator + 2*alpha*(1-alpha)*(I-(I-lr*A)**k)\n",
    "    return numerator/denominator * best_var_sgd(lr,A)\n",
    "\n",
    "def SGDexpweight(alpha,lr,A,x,t=1000): # Appendix A\n",
    "    I = np.identity(len(A))\n",
    "    return (I-lr*A)**t\n",
    "\n",
    "def SGDvarweight(alpha,lr,A,x,t=1000): # Appendix A\n",
    "    I = np.identity(len(A))\n",
    "    return (I-lr*A)**(2*t)*x + t * lr**2 * A**2 * np.linalg.inv(A)\n",
    "\n",
    "def lookaheadupdateweightexp(alpha, lr, A, k=5): # Lemma 1, eqn 10\n",
    "    I = np.identity(len(A))\n",
    "    return 1 - alpha + alpha*(I-lr*A)**k\n",
    "\n",
    "def lookaheadupdateweightvar(alpha, lr, A, k=5): # Lemma 1 eqn 11\n",
    "    I = np.identity(len(A))\n",
    "    p = (1-alpha + alpha*(I-lr*A)**k)**2\n",
    "    q = 0\n",
    "    matrix = I -lr*A\n",
    "    Sigma = np.linalg.inv(A)\n",
    "    for i in range(k):\n",
    "        q += matrix**(2*i) * lr**2 * A**2 * Sigma\n",
    "    return (p,q)\n",
    "    \n",
    "\n",
    "theta = [4,2]\n",
    "noisyquadloss(theta,A)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.05\n",
    "\n",
    "exp\n",
    "exploss = 0 # Eqn 5\n",
    "Sigma = np.linalg.inv(A)\n",
    "for i in range(dim):\n",
    "    exptheta = 0\n",
    "    vartheta = 0\n",
    "    exploss += 1/2 * A[i][i] * (exptheta**2+vartheta+Sigma[i][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
